{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhWCOAQeOiGD",
        "outputId": "44c8e7f6-0de0-4ec0-f377-38e4d4068c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "49k9-yDdO3z2"
      },
      "outputs": [],
      "source": [
        "# You can change the paths here.\n",
        "\n",
        "# Text files' folder path\n",
        "texts_folder_path = \"/content/drive/MyDrive/product_reviews\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtGm_kD-O-bH",
        "outputId": "9f71e9d1-ed3b-414b-a757-f342f3243d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMgMmN6WPjge",
        "outputId": "c21035e5-de51-404b-9c9e-685f45af65b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import download\n",
        "\n",
        "download(\"stopwords\")\n",
        "download(\"wordnet\")\n",
        "download(\"punkt\")\n",
        "download(\"omw-1.4\")\n",
        "download(\"averaged_perceptron_tagger\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aywPYdVeO_aE",
        "outputId": "a7f71e01-da4f-4be9-8a4f-90a2400c796e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Accuracy Percentage:  79.6\n",
            "Standard Deviation:  2.4979991993593593\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import copy\n",
        "import string\n",
        "import random\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.cluster import GAAClusterer\n",
        "\n",
        "\n",
        "files = [file for file in os.listdir(texts_folder_path) if file != \"README.txt\"]\n",
        "\n",
        "translational_table = str.maketrans(\n",
        "    \"\",\n",
        "    \"\",\n",
        "    (string.punctuation) + \"§―•\\t←→\",\n",
        ")\n",
        "\n",
        "# All possible stopwords\n",
        "stop_words = set(stopwords.words())\n",
        "\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "cores = multiprocessing.cpu_count()\n",
        "\n",
        "\n",
        "def pos_tagger(nltk_tag):\n",
        "    \"\"\"\n",
        "    Take a POS Tag, and return a wordnet equivalent tag to use for lemmatization\n",
        "    \"\"\"\n",
        "    if nltk_tag == None:\n",
        "        return None\n",
        "\n",
        "    if nltk_tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "final_processed_reviews = []\n",
        "processed_words = []\n",
        "for filename in files:\n",
        "    with open(texts_folder_path + \"/\" + filename, \"r\", encoding=\"utf-8-sig\") as file:\n",
        "        raw_text = file.read()\n",
        "\n",
        "    # Replacing the front slash with space\n",
        "    raw_lines = [\n",
        "        line for line in raw_text.lower().replace(\"/\", \" \").split(\"\\n\") if line != \"\"\n",
        "    ]\n",
        "\n",
        "    ## Retrieving reviews by sentences\n",
        "    reviews = []\n",
        "    t_separate = []\n",
        "    for i in range(len(raw_lines)):\n",
        "        sentence = raw_lines[i].split(\"##\")\n",
        "\n",
        "        if len(sentence) < 2:\n",
        "            reviews.append(copy.deepcopy(t_separate))\n",
        "            t_separate = []\n",
        "        else:\n",
        "            t_separate.append(sentence[1])\n",
        "\n",
        "        if i == len(raw_lines) - 1:\n",
        "            reviews.append(copy.deepcopy(t_separate))\n",
        "\n",
        "    reviews = [review for review in reviews if len(review) > 0]\n",
        "\n",
        "    # Processing Each Reviews\n",
        "    for review in reviews:\n",
        "        processed_lines = []\n",
        "        # Processing each sentence/line in review\n",
        "        for line in review:\n",
        "            tokens_with_punctuations = word_tokenize(line)\n",
        "\n",
        "            ############ POS TAGGING AND LEMMATIZATION ############\n",
        "            tokens_tags = pos_tag(tokens_with_punctuations)\n",
        "\n",
        "            # Preparing to lemmatize.\n",
        "            # Changing from POS Tags to WordNet Tags\n",
        "            wordnet_tags = [(x[0], pos_tagger(x[1])) for x in tokens_tags]\n",
        "\n",
        "            # Lemmatize with WordNet Lemmatizer\n",
        "            lemmatized_tokens = [\n",
        "                word if tag is None else wordnet_lemmatizer.lemmatize(word, tag)\n",
        "                for word, tag in wordnet_tags\n",
        "            ]\n",
        "            ####################################################################\n",
        "\n",
        "            # Removing stopwords\n",
        "            lemmatized_uni_tokens_without_sw = [\n",
        "                word for word in lemmatized_tokens if not word in stop_words\n",
        "            ]\n",
        "\n",
        "            # Removing punctuations, and other unnecessary tokens\n",
        "            lemmatized_without_punct = [\n",
        "                word.translate(translational_table)\n",
        "                for word in lemmatized_uni_tokens_without_sw\n",
        "                if word != \"\"\n",
        "                and word != \"'s\"\n",
        "                and word != \"'m\"\n",
        "                and word != \"'re\"\n",
        "                and word != \"'ve\"\n",
        "                and word != \"n't\"\n",
        "            ]\n",
        "\n",
        "            # Removing numerical tokens\n",
        "            lemmatized_without_punct = [\n",
        "                token\n",
        "                for token in lemmatized_without_punct\n",
        "                if token != \"\" and token.isnumeric() == False\n",
        "            ]\n",
        "\n",
        "            processed_lines += lemmatized_without_punct\n",
        "        processed_words += processed_lines\n",
        "        final_processed_reviews += [processed_lines]\n",
        "\n",
        "# Acquiring words and each respective counts\n",
        "word_counts = {}\n",
        "for word in processed_words:\n",
        "    if word in word_counts:\n",
        "        word_counts[word] += 1\n",
        "    else:\n",
        "        word_counts[word] = 1\n",
        "\n",
        "# Sorting\n",
        "sorted_words_counts = sorted(\n",
        "    [[key, value] for key, value in word_counts.items()], key=lambda val: val[1], reverse=True\n",
        ")\n",
        "\n",
        "# Acquiring top 50 words its counts\n",
        "top_50_words_counts = []\n",
        "i = 0\n",
        "while len(top_50_words_counts) < 50:\n",
        "    if \"\".join(reversed(sorted_words_counts[i][0])) != sorted_words_counts[i][0]:\n",
        "        top_50_words_counts.append(\n",
        "            (sorted_words_counts[i][0], sorted_words_counts[i][1])\n",
        "        )\n",
        "    i += 1\n",
        "\n",
        "# Top-50 words\n",
        "top_50_words = [item[0] for item in top_50_words_counts]\n",
        "# Pseudo made-up words\n",
        "reverse_top_50_words = [\"\".join(reversed(word)) for word in top_50_words]\n",
        "\n",
        "# Top 50 words plus its pseudo words\n",
        "one_hundred_words = top_50_words + reverse_top_50_words\n",
        "\n",
        "# Dictionary for top 50 words\n",
        "# Tracking counts with ->     word:count\n",
        "top_50_words_counts = {item[0]: item[1] for item in top_50_words_counts}\n",
        "\n",
        "percentages = []\n",
        "# Performing an experiment for 10 times from step 2 to step 5\n",
        "for i in range(10):\n",
        "    # Random sampling half of top 50 words in the corpus\n",
        "    replace_list = []\n",
        "    for word in top_50_words:\n",
        "        li = list(range(top_50_words_counts[word]))\n",
        "        random.shuffle(li)\n",
        "        replace_list.append(li[: top_50_words_counts[word] // 2])\n",
        "\n",
        "    # Deepcopying the processed reviews not to affect the final_processed_reviews\n",
        "    replaced_processed_reviews = copy.deepcopy(final_processed_reviews)\n",
        "\n",
        "    # Replacing random sampled words with made-up reverse words\n",
        "    for target in range(len(top_50_words)):\n",
        "        count = 0\n",
        "        for r in range(len(replaced_processed_reviews)):\n",
        "            for s in range(len(replaced_processed_reviews[r])):\n",
        "                if replaced_processed_reviews[r][s] == top_50_words[target]:\n",
        "                    if count in replace_list[target]:\n",
        "                        replaced_processed_reviews[r][s] = reverse_top_50_words[target]\n",
        "                    count += 1\n",
        "\n",
        "    # Setting up the parameters of the model one-by-one\n",
        "    w2v = Word2Vec(min_count=5, window=6, sg=1, size=160, workers=cores - 1)\n",
        "    # Building the vocabulary from a sequence of sentences\n",
        "    w2v.build_vocab(replaced_processed_reviews, progress_per=10000)\n",
        "    # Training the model\n",
        "    w2v.train(\n",
        "        replaced_processed_reviews,\n",
        "        total_examples=w2v.corpus_count,\n",
        "        epochs=20,\n",
        "        report_delay=1,\n",
        "    )\n",
        "\n",
        "    w2v.init_sims(replace=True)\n",
        "    w2v_vectors = w2v.wv.vectors\n",
        "    w2v_indices = {word: w2v.wv.vocab[word].index for word in w2v.wv.vocab}\n",
        "\n",
        "    feature_vecs = []\n",
        "    for word in one_hundred_words:\n",
        "        feature_vec = w2v_vectors[w2v_indices[word]]\n",
        "        feature_vecs.append(feature_vec)\n",
        "\n",
        "    # Clustering with Group Average Agglomerative clustering\n",
        "\n",
        "    clusterer = GAAClusterer(50)\n",
        "    labels = clusterer.cluster(feature_vecs, True)\n",
        "\n",
        "    # Calculating the percentage\n",
        "    p = 0\n",
        "    for i in range(50):\n",
        "        if labels[i] == labels[i + 50]:\n",
        "            p += 1\n",
        "    p = (p / 50) * 100\n",
        "    percentages.append(p)\n",
        "\n",
        "percentages = np.array(percentages)\n",
        "mean = np.mean(percentages)\n",
        "print(\"Average Accuracy Percentage: \", mean)\n",
        "std = np.std(percentages)\n",
        "print(\"Standard Deviation: \", std)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
