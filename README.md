# Distributional Semantics

## Method Description
Many considerations had done with pre-processing since good pre-processing measures were essential. Firstly, I go with the approach of retrieving sentences from each file. Since ## is used as a separator for each sentence, it was not difficult. After retrieving sentences, the front slash is replaced with whitespace as we want to separate a term like “macro/infinity” into “macro” and “infinity”. Then lines are split based on ## as a separator and collected under an array based on each review. After that, word_tokenize is used on each line of each review.

Furthermore, tabs are replaced with whitespaces to counteract some of the weird formatting. The text is already converted entirely to lowercase at this stage. Case insensitivity is usually seen as desirable, however, we are measuring a context-wise similarity here. Converting entirely into lowercase will have corrected some words as users might have typed the reviews inside the dataset with caps lock enabled.
After that, the nltk POS tagger is used for lemmatization, and all punctuations, undesirable symbols, and spoken forms such as “‘s” are removed. A helper function is used to reduce the code repetition as a function pos_tagger(self, nltk_tag: str), which takes a tag returned from nltk.pos_tag(). After all this, final_processed_reviews will contain tokens of all the lines of all reviews, while processed_words will have all of the words contained in those lines. On the topic of stop words, the set of stop words commonly used for all languages, accessible through nltk.stopwords.words(), was excluded from the list of tokens. Without removing stopwords will populate most 50 frequent words with only stop words. Removing numerics is also the right call during processing as removing the front slash also might affect the fraction such as “1/3” to “1” and “3”.

I used a library, word2vec, to build a Skip-Gram model, where the model is specified with min_count, window, and size. The model is specified to ignore all words with a total absolute frequency lower than 5, a window size being 6, and feature dimensions being 160. Skip-gram is used to predict the context word for a given target word. The model is trained with the retrieved_reviews to classify words into each context. Since the model considers the order of surrounding words during training, even the most fifty frequent words in the corpus are replaced with made-up reverse pseudowords, the most 50 words will have similar context classification. Being similar context classification the same, we can perform clustering, and theoretically should give the measure of relevancy of the target word and its corresponding pseudoword.

## Result Analysis
With using K-Means clustering, the mean accuracy is around 50 ± 3.0. After having done thorough research, with Agglomerative Clustering, the mean accuracy has drastically improved to 70 to 76 percent, with a mean standard deviation of around 3 to 3.3. Agglomerative Clustering uses a bottom-up approach where each of our 100 words starts in its own cluster. At each iteration, it merges the two clusters with the closest centroids until the desired number of clusters is achieved. On the other hand, K-Means did the opposite way of Agglomerative Clustering. In our case, the data space should look like it has 2 data points in very close proximity to each other, forming 50 clusters. One of the reasons GAAClusterer from nltk.cluster works well is that it uses cosine similarity instead of euclidean distance at which cosine similarity is generally preferred in context similarity. If the clustering algorithm from keras is using euclidean distance, then the mean accuracy is raised above 80.

In terms of hyperparameter settings, I set the window size to 6 as it gives the maximum accuracy. Smaller window sizes will give higher accuracy as they will tend to capture more about the word itself, not the context. The models trained with smaller sizes are susceptible to noise. Larger windows tend to capture more domain information, thus increasing the quality of the learned model. Setting the right amount of minimum count of words to be considered as contexts is important. Words that appear only once or twice in a corpus are probably uninteresting typos and garbage and should be ignored as there is not enough data to make any meaningful training on those words.

According to current hyperparameters and text preprocessing methods gives around 79-80 percent mean accuracy with standard deviations from 2.5 - 2.9. In trials with different feature sizes such as 100, 160, and 200, the less dimension of features could hurt the accuracy, though does not make any more difference after 160. Window size 6 gives the maximum accuracy from my hyperparameter testing. Higher epochs hurts the accuracy as the model starts memorizing the data.