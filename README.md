# Distributional Semantics

## Method Description
Many considerations had done with pre-processing since good pre-processing measures were essential. Firstly, I go with the approach of retrieving sentences from each file. Since ## is used as a separator for each sentence, it was not difficult. After retrieving sentences, the front slash is replaced with whitespace as we want to separate a term like “macro/infinity” into “macro” and “infinity”. Then lines are split based on ## as a separator and collected under an array based on each review. After that, word_tokenize is used on each line of each review.

Furthermore, tabs are replaced with whitespaces to counteract some of the weird formatting. The text is already converted entirely to lowercase at this stage. Case insensitivity is usually seen as desirable, however, we are measuring a context-wise similarity here. Converting entirely into lowercase will have corrected some words as users might have typed the reviews inside the dataset with caps lock enabled.
After that, the nltk POS tagger is used for lemmatization, and all punctuations, undesirable symbols, and spoken forms such as “‘s” are removed. A helper function is used to reduce the code repetition as a function pos_tagger(self, nltk_tag: str), which takes a tag returned from nltk.pos_tag(). After all this, final_processed_reviews will contain tokens of all the lines of all reviews, while processed_words will have all of the words contained in those lines. On the topic of stop words, the set of stop words commonly used for all languages, accessible through nltk.stopwords.words(), was excluded from the list of tokens. Without removing stopwords will populate most 50 frequent words with only stop words. Removing numerics is also the right call during processing as removing the front slash also might affect the fraction such as “1/3” to “1” and “3”.

I used a library, word2vec, to build a Skip-Gram model, where the model is specified with min_count, window, and size. The model is specified to ignore all words with a total absolute frequency lower than 5, a window size being 6, and feature dimensions being 160. Skip-gram is used to predict the context word for a given target word. The model is trained with the retrieved_reviews to classify words into each context. Since the model considers the order of surrounding words during training, even the most fifty frequent words in the corpus are replaced with made-up reverse pseudowords, the most 50 words will have similar context classification. Being similar context classification the same, we can perform clustering, and theoretically should give the measure of relevancy of the target word and its corresponding pseudoword.
